import os
import sys
import time
import threading
import itertools
import textwrap
import faiss
import numpy as np
from llama_cpp import Llama
from sklearn.feature_extraction.text import TfidfVectorizer
import re
from collections import Counter
import difflib
from sklearn.metrics.pairwise import cosine_similarity
import unicodedata
import string

# === CONFIGURATION ===
MODEL_PATH = "./model/Qwen3-1.7B-Q4_K_M.gguf"
N_CTX = 8192
N_THREADS = 8
LOG_DIR = "./logs_apache"
MAX_LOG_LINES = 200
CHUNK_SIZE = 50
TOP_K_CHUNKS = 3

vectorizer = TfidfVectorizer()

def chunk_logs(logs_dict, chunk_size=CHUNK_SIZE):
    chunks = []
    for fname, lines in logs_dict.items():
        for i in range(0, len(lines), chunk_size):
            chunk = "\n".join(lines[i:i+chunk_size])
            chunks.append(chunk)
    return chunks

def build_faiss_index(chunks):
    tfidf_matrix = vectorizer.fit_transform(chunks)
    embeddings = tfidf_matrix.toarray().astype(np.float32)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

def retrieve_relevant_chunks(question, chunks, index, top_k=TOP_K_CHUNKS):
    q_vec = vectorizer.transform([question]).toarray().astype(np.float32)
    D, I = index.search(q_vec, top_k)
    return [chunks[i] for i in I[0]]

def read_logfile(filename, max_lines=MAX_LOG_LINES):
    path = os.path.join(LOG_DIR, filename)
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
            return lines[-max_lines:] if max_lines else lines
    except Exception:
        return []

def get_all_logs():
    logs = {}
    if not os.path.isdir(LOG_DIR):
        print(f"‚ùå Le dossier {LOG_DIR} n'existe pas.")
        return logs
    for fname in os.listdir(LOG_DIR):
        if fname.endswith(".log"):
            logs[fname] = read_logfile(fname)
    return logs

class Spinner:
    def __init__(self, message="‚è≥ Le mod√®le r√©fl√©chit "):
        self.spinner = itertools.cycle(['|', '/', '-', '\\'])
        self.stop_running = False
        self.message = message
        self.thread = threading.Thread(target=self.animate)

    def animate(self):
        while not self.stop_running:
            sys.stdout.write(f"\r{self.message}{next(self.spinner)}")
            sys.stdout.flush()
            time.sleep(0.1)
        sys.stdout.write('\r' + ' ' * (len(self.message) + 2) + '\r')

    def start(self):
        self.thread.start()

    def stop(self):
        self.stop_running = True
        self.thread.join()

def main():
    print(f"‚è≥ Chargement du mod√®le depuis {MODEL_PATH} ...")
    try:
        llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=N_CTX,
            n_threads=N_THREADS,
            verbose=False
        )
        print("‚úÖ Mod√®le charg√©.")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        sys.exit(1)

    print("\nüß† Pose ta question ('/log <question>' pour analyser les logs, 'exit' pour quitter):\n")

    while True:
        try:
            user_input = input("üë§ Vous: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nüëã Fin de session.")
            break

        if not user_input:
            print("‚ùó Veuillez entrer une question.")
            continue
        if user_input.lower() in ["exit", "quit"]:
            print("üëã √Ä bient√¥t !")
            break

        if user_input.startswith("/log "):
            question = user_input[len("/log "):].strip()
            logs = get_all_logs()
            if not logs:
                print("‚ùå Aucun log disponible pour r√©pondre.")
                continue
            chunks = chunk_logs(logs)
            if not chunks:
                print("‚ùå Les logs sont vides.")
                continue
            index = build_faiss_index(chunks)
            top_chunks = retrieve_relevant_chunks(question, chunks, index)
            context = "\n---\n".join(top_chunks)

            prompt = (
                f"Tu es un assistant expert qui analyse des logs Apache.\n"
                f"Voici des extraits pertinents des logs :\n\n{context}\n\n"
                f"Question : {question}\n\n"
                f"R√©ponds librement avec ta propre logique."
            )
            max_tokens = 256
            temperature = 0.5
        else:
            prompt = user_input
            max_tokens = 128
            temperature = 0.3

        spinner = Spinner()
        spinner.start()
        try:
            output = llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            spinner.stop()

            response = output.get("choices", [{}])[0].get("text", "").strip()
            if not response:
                response = "Je ne trouve pas la r√©ponse."

            print("\nü§ñ Qwen:")
            print(textwrap.fill(response, width=100))
            print()
        except Exception as e:
            spinner.stop()
            print(f"\n‚ùå Erreur durant l'inf√©rence : {e}")

# Chemin vers les logs Apache
LOGS_DIR = 'logs_apache/'

# Regex pour extraire une IP (IPv4 ou IPv6 simplifi√©)
IP_REGEX = re.compile(r'((?:\d{1,3}\.){3}\d{1,3}|[a-fA-F0-9:]+)')

def extract_ip(line):
    match = IP_REGEX.match(line.strip())
    if match:
        return match.group(1)
    return None

# Charger tous les logs dans une seule liste de lignes
log_lines = []
for filename in os.listdir(LOGS_DIR):
    if filename.endswith('.txt') or filename.endswith('.log'):
        with open(os.path.join(LOGS_DIR, filename), 'r', encoding='utf-8', errors='ignore') as f:
            log_lines.extend(f.readlines())

# Fonctions d'analyse des logs

def count_total_accesses():
    return len(log_lines)

def count_status_code(code):
    count = 0
    for line in log_lines:
        match = re.search(r'" (\d{3}) ', line)
        if match and match.group(1) == str(code):
            count += 1
    return count

def most_active_ip():
    ips = [extract_ip(line) for line in log_lines if extract_ip(line)]
    if not ips:
        return None
    counter = Counter(ips)
    return counter.most_common(1)[0]

def unique_ips():
    ips = set(extract_ip(line) for line in log_lines if extract_ip(line))
    return len(ips)

def top_n_ips(n=5):
    ips = [extract_ip(line) for line in log_lines if extract_ip(line)]
    counter = Counter(ips)
    return counter.most_common(n)

# === MAPPING QUESTIONS.TXT ===
QUESTIONS_MAP = {
    "Quelles sont les pages \"not found\" (erreurs 404) ?": lambda: pages_not_found_404(),
    "Y a-t-il des erreurs 500 dans les logs ?": lambda: errors_500_present(),
    "Quels sont les IPs qui font le plus de requ√™tes ?": lambda: top_n_ips(5),
    "Quels sont les URL les plus demand√©s ?": lambda: most_requested_urls(5),
    "Y a-t-il des tentatives d'acc√®s suspectes ou erreurs d'authentification ?": lambda: suspicious_access_or_auth_errors(),
    "Quels sont les pics d'activit√© sur le serveur ?": lambda: activity_peaks(),
    "Combien de requ√™tes par minute/heures ?": lambda: requests_per_time(),
    "Quelle est la taille moyenne des r√©ponses ?": lambda: average_response_size(),
    "Y a-t-il des erreurs de timeout ?": lambda: timeout_errors_present(),
    "Quel est le temps moyen de r√©ponse ?": lambda: average_response_time(),
    "Y a-t-il des redirections (301, 302) fr√©quentes ?": lambda: frequent_redirections(),
    "Liste-moi les requ√™tes avec des codes d'√©tat HTTP diff√©rents de 200.": lambda: non_200_requests(),
}

# === FONCTIONS D'ANALYSE POUR CHAQUE QUESTION ===
def pages_not_found_404():
    urls = []
    for line in log_lines:
        match = re.search(r'"(GET|POST|HEAD|PUT|DELETE|OPTIONS) ([^ ]+) [^"]+" 404', line)
        if match:
            urls.append(match.group(2))
    if not urls:
        return "Aucune page 404 trouv√©e."
    return "Pages 'not found' (404) :\n" + '\n'.join(set(urls))

def errors_500_present():
    count = count_status_code(500)
    return f"Il y a {count} erreur(s) 500 dans les logs." if count else "Aucune erreur 500 trouv√©e."

def most_requested_urls(n=5):
    urls = []
    for line in log_lines:
        match = re.search(r'"(GET|POST|HEAD|PUT|DELETE|OPTIONS) ([^ ]+) [^"]+" \d{3}', line)
        if match:
            urls.append(match.group(2))
    if not urls:
        return "Aucune URL trouv√©e."
    counter = Counter(urls)
    return "URLs les plus demand√©es :\n" + '\n'.join([f"{url} ({count} requ√™tes)" for url, count in counter.most_common(n)])

def suspicious_access_or_auth_errors():
    suspicious = []
    for line in log_lines:
        if re.search(r'401|403|denied|unauthorized|forbidden|login|auth', line, re.IGNORECASE):
            suspicious.append(line.strip())
    if not suspicious:
        return "Aucune tentative suspecte ou erreur d'authentification trouv√©e."
    return f"{len(suspicious)} acc√®s suspects ou erreurs d'authentification d√©tect√©s."

def activity_peaks():
    # Placeholder simple : √† am√©liorer pour d√©tecter de vrais pics
    return "D√©tection de pics d'activit√© non impl√©ment√©e (√† coder selon la granularit√© temporelle des logs)."

def requests_per_time():
    # Placeholder simple : √† am√©liorer pour calculer par minute/heure
    return "Calcul du nombre de requ√™tes par minute/heure non impl√©ment√© (√† coder selon le format de date des logs)."

def average_response_size():
    sizes = []
    for line in log_lines:
        match = re.search(r'" \d{3} (\d+)', line)
        if match:
            sizes.append(int(match.group(1)))
    if not sizes:
        return "Impossible de calculer la taille moyenne des r√©ponses."
    avg = sum(sizes) / len(sizes)
    return f"Taille moyenne des r√©ponses : {avg:.2f} octets."

def timeout_errors_present():
    count = sum(1 for line in log_lines if 'timeout' in line.lower())
    return f"{count} erreur(s) de timeout d√©tect√©e(s)." if count else "Aucune erreur de timeout trouv√©e."

def average_response_time():
    # Placeholder : √† adapter selon la pr√©sence du temps de r√©ponse dans les logs
    return "Calcul du temps moyen de r√©ponse non impl√©ment√© (√† coder selon le format des logs)."

def frequent_redirections():
    count_301 = count_status_code(301)
    count_302 = count_status_code(302)
    total = count_301 + count_302
    if total == 0:
        return "Aucune redirection 301 ou 302 trouv√©e."
    return f"Redirections fr√©quentes : {count_301} (301), {count_302} (302)"

def non_200_requests():
    reqs = []
    for line in log_lines:
        match = re.search(r'"(GET|POST|HEAD|PUT|DELETE|OPTIONS) ([^ ]+) [^"]+" (?!200)\d{3}', line)
        if match:
            reqs.append(f"{match.group(1)} {match.group(2)}")
    if not reqs:
        return "Toutes les requ√™tes ont un code 200."
    return "Requ√™tes avec code diff√©rent de 200 :\n" + '\n'.join(reqs)

# Charger les questions de questions.txt
with open('questions.txt', encoding='utf-8') as f:
    QUESTIONS_LIST = [q.strip() for q in f if q.strip()]

# Vectoriser les questions
vectorizer_q = TfidfVectorizer().fit(QUESTIONS_LIST)
questions_vecs = vectorizer_q.transform(QUESTIONS_LIST)

SIMILARITY_THRESHOLD = 0.4  # Plus tol√©rant
FUZZY_THRESHOLD = 0.7       # Pour le ratio difflib

# Fonction de nettoyage avanc√© de texte
def clean_text(text):
    text = text.strip().lower()
    # Normalisation unicode
    text = ''.join(
        c for c in unicodedata.normalize('NFD', text)
        if unicodedata.category(c) != 'Mn'
    )
    # Remplacement apostrophes et guillemets
    text = text.replace('‚Äô', "'").replace('"', '"').replace('"', '"')
    # Remplacement espaces ins√©cables
    text = text.replace('\xa0', ' ').replace('\u202f', ' ')
    # Suppression ponctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Suppression espaces multiples
    text = ' '.join(text.split())
    return text

# Nettoyer les questions pour le matching
CLEANED_QUESTIONS_LIST = [clean_text(q) for q in QUESTIONS_LIST]

# Adapter le vectorizer sur les questions nettoy√©es
vectorizer_q = TfidfVectorizer().fit(CLEANED_QUESTIONS_LIST)
questions_vecs = vectorizer_q.transform(CLEANED_QUESTIONS_LIST)

def find_best_question(user_question):
    cleaned = clean_text(user_question)
    user_vec = vectorizer_q.transform([cleaned])
    sims = cosine_similarity(user_vec, questions_vecs)[0]
    best_idx = sims.argmax()
    if sims[best_idx] >= SIMILARITY_THRESHOLD:
        return QUESTIONS_LIST[best_idx], best_idx, sims[best_idx]
    # Fallback : fuzzy matching
    ratios = [difflib.SequenceMatcher(None, cleaned, cq).ratio() for cq in CLEANED_QUESTIONS_LIST]
    fuzzy_idx = max(range(len(ratios)), key=lambda i: ratios[i])
    if ratios[fuzzy_idx] >= FUZZY_THRESHOLD:
        return QUESTIONS_LIST[fuzzy_idx], fuzzy_idx, ratios[fuzzy_idx]
    return None, None, max(sims[best_idx], ratios[fuzzy_idx])

# === NOUVELLE BOUCLE CLI ===
if __name__ == "__main__":
    print("Chatbot Apache pr√™t ! Tapez 'quit' pour arr√™ter.")
    print("Vous pouvez poser les questions suivantes (ou des variantes proches) :")
    questions_to_hide = [
        "Y a-t-il des tentatives d'acc√®s suspectes ou erreurs d'authentification ?",
        "Liste-moi les requ√™tes avec des codes d'√©tat HTTP diff√©rents de 200.",
        "Quels sont les pics d'activit√© sur le serveur ?",
        "Combien de requ√™tes par minute/heures ?"
    ]
    for q in QUESTIONS_MAP:
        if q not in questions_to_hide:
            print(f"- {q}")
    last_suggestion_idx = None
    while True:
        message = input("Vous : ").strip()
        if message.lower() == 'quit':
            break
        if message.lower() in ['oui', 'yes', 'ok'] and last_suggestion_idx is not None:
            suggestion = QUESTIONS_LIST[last_suggestion_idx]
            print(f"(J'ex√©cute la question sugg√©r√©e : '{suggestion}')")
            response = QUESTIONS_MAP[suggestion]()
            last_suggestion_idx = None
        else:
            best_q, best_idx, score = find_best_question(message)
            if best_q and best_q in QUESTIONS_MAP:
                print(f"(J'ai compris : '{best_q}')")
                response = QUESTIONS_MAP[best_q]()
                last_suggestion_idx = None
            else:
                # Proposer la question la plus proche trouv√©e (par index)
                if questions_vecs.shape[0] > 0:
                    cleaned = clean_text(message)
                    user_vec = vectorizer_q.transform([cleaned])
                    sims = cosine_similarity(user_vec, questions_vecs)[0]
                    best_idx = sims.argmax()
                    suggestion = QUESTIONS_LIST[best_idx]
                    response = (
                        "Je ne comprends pas bien la question. "
                        f"Voulais-tu dire : '{suggestion}' ?\n"
                        "(R√©ponds 'oui' pour que j'ex√©cute cette question, ou reformule ta demande.)"
                    )
                    last_suggestion_idx = best_idx
                else:
                    response = "Je ne comprends que les questions du fichier questions.txt ou des variantes proches."
                    last_suggestion_idx = None
        print(f"Bot : {response}")
